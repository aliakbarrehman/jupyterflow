{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JupyterFlow \u00b6 Run your workflow on JupyterHub! What is JupyterFlow? \u00b6 Run Argo Workflow on JupyterHub with single command. No Kubernetes knowledge (YAML) needed to run. No container image build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. This project only works on JupyterHub for Kubernetes . The following jupyterflow command will make sequence workflow. That's it! jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file. Problem to solve \u00b6 I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who were not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem. Run your workflow on JupyterHub with single command without Kubernetes & container troublesome task. Getting Started \u00b6 To set up jupyterflow and start running your first workflow, follow the Getting Started guide. How does it work \u00b6 To learn how it works, go to How it works guide. Examples \u00b6 For examples how to use, please see Examples page. Workflow file Configuration \u00b6 To find out more configuration, take a look at Configuration page.","title":"Overview"},{"location":"#jupyterflow","text":"Run your workflow on JupyterHub!","title":"JupyterFlow"},{"location":"#what-is-jupyterflow","text":"Run Argo Workflow on JupyterHub with single command. No Kubernetes knowledge (YAML) needed to run. No container image build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. This project only works on JupyterHub for Kubernetes . The following jupyterflow command will make sequence workflow. That's it! jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file.","title":"What is JupyterFlow?"},{"location":"#problem-to-solve","text":"I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who were not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem. Run your workflow on JupyterHub with single command without Kubernetes & container troublesome task.","title":"Problem to solve"},{"location":"#getting-started","text":"To set up jupyterflow and start running your first workflow, follow the Getting Started guide.","title":"Getting Started"},{"location":"#how-does-it-work","text":"To learn how it works, go to How it works guide.","title":"How does it work"},{"location":"#examples","text":"For examples how to use, please see Examples page.","title":"Examples"},{"location":"#workflow-file-configuration","text":"To find out more configuration, take a look at Configuration page.","title":"Workflow file Configuration"},{"location":"configuration/","text":"Configuring workflow \u00b6 workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs. Minimum description \u00b6 Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again Full description \u00b6 Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately","title":"Configuring workflow"},{"location":"configuration/#configuring-workflow","text":"workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs.","title":"Configuring workflow"},{"location":"configuration/#minimum-description","text":"Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again","title":"Minimum description"},{"location":"configuration/#full-description","text":"Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately","title":"Full description"},{"location":"get-started/","text":"Get Started \u00b6 Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes. 1. Install Kubernetes \u00b6 Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes. 2. Install JupyterHub \u00b6 Also, follow the Zero to JupyterHub instruction to set up JupyterHub . There is three things you should configure while installing jupyterflow. 1) Specify serviceAccoutName \u00b6 Find singleuser property and specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account a role to create Workflow object. # config.yaml singleuser : serviceAccountName : default 2) Add label \u00b6 Find singleuser.extraLabels property and add jupyterflow/username: \"{username}\" label in config.yaml . This label will be used to find my notebook server Pod . # config.yaml singleuser : extraLabels : jupyterflow/username : \"{username}\" 3) Setting Storage \u00b6 To use the same JupyterHub home directory as in Argo Workflow, Configure singleuser.storage property. To run jobs on multiple different node, you should use ReadWriteMany access mode type storage, such as nfs-server-provisioner . If you're unfamiliar with storage access mode, take a look at Kubernetes persistent volume access mode . Configuring singleuser storage access mode as ReadWriteOnce is perfectly fine, but bear in mind that your jobs will be run on only one node that your jupyter notebook is mounted. # config.yaml singleuser : storage : type : dynamic # or static dynamic : storageClass : nfs-server # For example, nfs-server-provisioner storageAccessModes : [ ReadWriteMany ] # Make sure your volume supports ReadWriteMany for running distributed jobs. static : pvcName : my-static-pvc # Static pvc also works fine. # Also static pvc should support ReadWriteMany mode for distributed jobs. 3. Install Argo Workflow \u00b6 Install Argo workflow with this page . You need to install Argo workflow in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for JupyterHub and Argo Workflow. # create namespace jupyterflow kubectl create ns jupyterflow # install jupyterhub in jupyterflow helm install jupyterhub jupyterhub/jupyterhub --namespace jupyterflow # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml 4. Expose Argo Workflow UI \u00b6 Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow . 5. Grant JupyterHub Service Account RBAC \u00b6 Grant the service account used in JupyterHub a role to create Argo Workflow objects. Options 1) \u00b6 The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default Options 2) \u00b6 For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default service account with worflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ -n jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts . 6. Install jupyterflow \u00b6 Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow 7. Run my first workflow \u00b6 Refer to examples/get-started by command \u00b6 Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow. by workflow.yaml file \u00b6 If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your own workflow file (for example, workflow.yaml , the name doen't matter) For more information, check out Configuring workflow # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Get Started"},{"location":"get-started/#get-started","text":"Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes.","title":"Get Started"},{"location":"get-started/#1-install-kubernetes","text":"Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes.","title":"1. Install Kubernetes"},{"location":"get-started/#2-install-jupyterhub","text":"Also, follow the Zero to JupyterHub instruction to set up JupyterHub . There is three things you should configure while installing jupyterflow.","title":"2. Install JupyterHub"},{"location":"get-started/#1-specify-serviceaccoutname","text":"Find singleuser property and specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account a role to create Workflow object. # config.yaml singleuser : serviceAccountName : default","title":"1) Specify serviceAccoutName"},{"location":"get-started/#2-add-label","text":"Find singleuser.extraLabels property and add jupyterflow/username: \"{username}\" label in config.yaml . This label will be used to find my notebook server Pod . # config.yaml singleuser : extraLabels : jupyterflow/username : \"{username}\"","title":"2) Add label"},{"location":"get-started/#3-setting-storage","text":"To use the same JupyterHub home directory as in Argo Workflow, Configure singleuser.storage property. To run jobs on multiple different node, you should use ReadWriteMany access mode type storage, such as nfs-server-provisioner . If you're unfamiliar with storage access mode, take a look at Kubernetes persistent volume access mode . Configuring singleuser storage access mode as ReadWriteOnce is perfectly fine, but bear in mind that your jobs will be run on only one node that your jupyter notebook is mounted. # config.yaml singleuser : storage : type : dynamic # or static dynamic : storageClass : nfs-server # For example, nfs-server-provisioner storageAccessModes : [ ReadWriteMany ] # Make sure your volume supports ReadWriteMany for running distributed jobs. static : pvcName : my-static-pvc # Static pvc also works fine. # Also static pvc should support ReadWriteMany mode for distributed jobs.","title":"3) Setting Storage"},{"location":"get-started/#3-install-argo-workflow","text":"Install Argo workflow with this page . You need to install Argo workflow in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for JupyterHub and Argo Workflow. # create namespace jupyterflow kubectl create ns jupyterflow # install jupyterhub in jupyterflow helm install jupyterhub jupyterhub/jupyterhub --namespace jupyterflow # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml","title":"3. Install Argo Workflow"},{"location":"get-started/#4-expose-argo-workflow-ui","text":"Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow .","title":"4. Expose Argo Workflow UI"},{"location":"get-started/#5-grant-jupyterhub-service-account-rbac","text":"Grant the service account used in JupyterHub a role to create Argo Workflow objects.","title":"5. Grant JupyterHub Service Account RBAC"},{"location":"get-started/#options-1","text":"The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default","title":"Options 1)"},{"location":"get-started/#options-2","text":"For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default service account with worflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ -n jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts .","title":"Options 2)"},{"location":"get-started/#6-install-jupyterflow","text":"Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow","title":"6. Install jupyterflow"},{"location":"get-started/#7-run-my-first-workflow","text":"Refer to examples/get-started","title":"7. Run my first workflow"},{"location":"get-started/#by-command","text":"Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow.","title":"by command"},{"location":"get-started/#by-workflowyaml-file","text":"If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your own workflow file (for example, workflow.yaml , the name doen't matter) For more information, check out Configuring workflow # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"by workflow.yaml file"},{"location":"how-it-works/","text":"How it works \u00b6 jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data) using given service account. With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code might help you understand how jupyterflow works. # collect meta data of current environment. notebook_pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build workflow based on meta data and user workflow information. workflow_spec = build_workflow ( notebook_pod_spec , user_workflow_file ) # create Argo workflow. response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data) using given service account. With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code might help you understand how jupyterflow works. # collect meta data of current environment. notebook_pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build workflow based on meta data and user workflow information. workflow_spec = build_workflow ( notebook_pod_spec , user_workflow_file ) # create Argo workflow. response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"examples/","text":"Examples \u00b6 Basic \u00b6 Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook. ML Pipeline \u00b6 ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#basic","text":"Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook.","title":"Basic"},{"location":"examples/#ml-pipeline","text":"ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"ML Pipeline"},{"location":"examples/basic/","text":"Basic \u00b6 Run by command \u00b6 Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple execution. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow. Run by workflow.yaml \u00b6 Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - ls - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Basic"},{"location":"examples/basic/#basic","text":"","title":"Basic"},{"location":"examples/basic/#run-by-command","text":"Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple execution. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow.","title":"Run by command"},{"location":"examples/basic/#run-by-workflowyaml","text":"Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - ls - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Run by workflow.yaml"},{"location":"examples/ml-pipeline/","text":"ML Pipeline \u00b6 Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for testing purpose. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python input.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"},{"location":"examples/ml-pipeline/#ml-pipeline","text":"Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for testing purpose. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python input.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"}]}