{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JupyterFlow \u00b6 Run your workflow on JupyterHub! What is JupyterFlow? \u00b6 Run Argo Workflow on JupyterHub with single command. No Kubernetes knowledge (YAML) needed to run. No container image build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. This project only works on JupyterHub for Kubernetes . The following jupyterflow command will make sequence workflow. That's it! jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file. Problem to solve \u00b6 I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who were not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem. Run your workflow on JupyterHub with single command without Kubernetes & container troublesome task. Getting Started \u00b6 To set up jupyterflow and start running your first workflow, follow the Getting Started guide. How does it work \u00b6 To learn how it works, go to How it works guide. Examples \u00b6 For examples how to use, please see Examples page. Configuration \u00b6 To find out more configuration, take a look at Configuration page. CLI Reference \u00b6 For more detail usage of jupyterflow command line interface, find out more at CLI Reference page.","title":"Overview"},{"location":"#jupyterflow","text":"Run your workflow on JupyterHub!","title":"JupyterFlow"},{"location":"#what-is-jupyterflow","text":"Run Argo Workflow on JupyterHub with single command. No Kubernetes knowledge (YAML) needed to run. No container image build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. This project only works on JupyterHub for Kubernetes . The following jupyterflow command will make sequence workflow. That's it! jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file.","title":"What is JupyterFlow?"},{"location":"#problem-to-solve","text":"I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who were not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem. Run your workflow on JupyterHub with single command without Kubernetes & container troublesome task.","title":"Problem to solve"},{"location":"#getting-started","text":"To set up jupyterflow and start running your first workflow, follow the Getting Started guide.","title":"Getting Started"},{"location":"#how-does-it-work","text":"To learn how it works, go to How it works guide.","title":"How does it work"},{"location":"#examples","text":"For examples how to use, please see Examples page.","title":"Examples"},{"location":"#configuration","text":"To find out more configuration, take a look at Configuration page.","title":"Configuration"},{"location":"#cli-reference","text":"For more detail usage of jupyterflow command line interface, find out more at CLI Reference page.","title":"CLI Reference"},{"location":"cli-ref/","text":"CLI Reference \u00b6 jupyterflow is the command line interface for JupyterFlow jupyterflow run \u00b6 To run a workflow to Argo Workflow on JupyterHub Synopsis \u00b6 jupyterflow run [ flags ] Options \u00b6 -h, --help help for list -c, --command string Command to run workflow. ex) `jupyterflow run -c \"python main.py >> python next.py\"` -f, --filename string Path for workflow.yaml file. ex) `jupyterflow run -f workflow.yaml` -o, --output string Output format. (default is `-o jsonpath=\"metadata.name\"`, other possible options are yaml, json, jsonpath.) --dry-run Only prints Argo Workflow object, without accually sending it. jupyterflow config \u00b6 View or create JupyterFlow configuration file to override Argo Workflow object specification. For detail information, refer to JupyterFlow Configuration Synopsis \u00b6 jupyterflow config [ flags ] Options \u00b6 -h, --help help for list --generate-config Generates default `$HOME/.jupyterflow.yaml` configuration file.","title":"CLI Reference"},{"location":"cli-ref/#cli-reference","text":"jupyterflow is the command line interface for JupyterFlow","title":"CLI Reference"},{"location":"cli-ref/#jupyterflow-run","text":"To run a workflow to Argo Workflow on JupyterHub","title":"jupyterflow run"},{"location":"cli-ref/#synopsis","text":"jupyterflow run [ flags ]","title":"Synopsis"},{"location":"cli-ref/#options","text":"-h, --help help for list -c, --command string Command to run workflow. ex) `jupyterflow run -c \"python main.py >> python next.py\"` -f, --filename string Path for workflow.yaml file. ex) `jupyterflow run -f workflow.yaml` -o, --output string Output format. (default is `-o jsonpath=\"metadata.name\"`, other possible options are yaml, json, jsonpath.) --dry-run Only prints Argo Workflow object, without accually sending it.","title":"Options"},{"location":"cli-ref/#jupyterflow-config","text":"View or create JupyterFlow configuration file to override Argo Workflow object specification. For detail information, refer to JupyterFlow Configuration","title":"jupyterflow config"},{"location":"cli-ref/#synopsis_1","text":"jupyterflow config [ flags ]","title":"Synopsis"},{"location":"cli-ref/#options_1","text":"-h, --help help for list --generate-config Generates default `$HOME/.jupyterflow.yaml` configuration file.","title":"Options"},{"location":"configuration/","text":"Configuration \u00b6 workflow.yaml Configuration \u00b6 workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs. The name and the path does not matter as long as you pass to run -f option argument. Minimum description \u00b6 Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again jupyterflow run -f workflow.yaml Full description \u00b6 Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again cmd_mode : exec # shell dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required cmd_mode Choose to run image in exec or shell form. Optional exec dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately exec vs shell \u00b6 In exec mode, your command will be executed as [\"echo\", \"hello\", \"world\"] . In shell mode, your command will be executed as [\"/bin/sh\", \"-c\", \"echo hello world\"] . In exec mode, the command is more straightforward since there is no shell process involved and it is being called directly. In shell mode, you can fully utilize the power of shell, such as shell script commands. ( >> , && and so on.) Jupyterflow Configuration \u00b6 You can override Argo Workflow spec by configuring $HOME/.jupyterflow.yaml file. This file path can be changed by setting JUPYTERFLOW_CONFIG_FILE environment variable. ( export JUPYTERFLOW_CONFIG_FILE=/tmp/myjupyterflow.yaml ) The following command will create .jupyterflow.yaml on $HOME directory. jupyterflow config --generate-config # jupyterflow config file created. cat $HOME /.jupyterflow.yaml # spec: # image: jupyter/datascience-notebook:latest # imagePullPolicy: Always # imagePullSecrets: # - name: \"default\" # env: # - name: \"CUSTOM_KEY\" # value: \"CUSTOM_VAL\" # resources: # requests: # cpu: 500m # memory: 500Mi # limits: # cpu: 500m # memory: 500Mi # nodeSelector: {} # runAsUser: 1000 # runAsGroup: 100 # serviceAccountName: default # volumes: # - name: nas001 # persistentVolumeClaim: # claimName: nas001 # volumeMounts: # - name: nas001 # mountPath: /nas001 Umcomment the property you want to override. For example, if you want your workflow jobs to run on GPU nodes, configure spec.resources or spec.nodeSelector property. spec: # image: jupyter/datascience-notebook:latest # imagePullPolicy: Always # imagePullSecrets: # - name: \"default\" # env: # - name: \"CUSTOM_KEY\" # value: \"CUSTOM_VAL\" resources: requests: cpu: 500m memory: 500Mi nvidia.com/gpu: 1 limits: cpu: 500m memory: 500Mi nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-node # runAsUser: 1000 # runAsGroup: 100 # serviceAccountName: default # volumes: # - name: nas001 # persistentVolumeClaim: # claimName: nas001 # volumeMounts: # - name: nas001 # mountPath: /nas001 Run jupyterflow and check out the result whether your workflow has run on GPU nodes. jupyterflow run -f workflow.yaml","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#workflowyaml-configuration","text":"workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs. The name and the path does not matter as long as you pass to run -f option argument.","title":"workflow.yaml Configuration"},{"location":"configuration/#minimum-description","text":"Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again jupyterflow run -f workflow.yaml","title":"Minimum description"},{"location":"configuration/#full-description","text":"Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again cmd_mode : exec # shell dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required cmd_mode Choose to run image in exec or shell form. Optional exec dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately","title":"Full description"},{"location":"configuration/#exec-vs-shell","text":"In exec mode, your command will be executed as [\"echo\", \"hello\", \"world\"] . In shell mode, your command will be executed as [\"/bin/sh\", \"-c\", \"echo hello world\"] . In exec mode, the command is more straightforward since there is no shell process involved and it is being called directly. In shell mode, you can fully utilize the power of shell, such as shell script commands. ( >> , && and so on.)","title":"exec vs shell"},{"location":"configuration/#jupyterflow-configuration","text":"You can override Argo Workflow spec by configuring $HOME/.jupyterflow.yaml file. This file path can be changed by setting JUPYTERFLOW_CONFIG_FILE environment variable. ( export JUPYTERFLOW_CONFIG_FILE=/tmp/myjupyterflow.yaml ) The following command will create .jupyterflow.yaml on $HOME directory. jupyterflow config --generate-config # jupyterflow config file created. cat $HOME /.jupyterflow.yaml # spec: # image: jupyter/datascience-notebook:latest # imagePullPolicy: Always # imagePullSecrets: # - name: \"default\" # env: # - name: \"CUSTOM_KEY\" # value: \"CUSTOM_VAL\" # resources: # requests: # cpu: 500m # memory: 500Mi # limits: # cpu: 500m # memory: 500Mi # nodeSelector: {} # runAsUser: 1000 # runAsGroup: 100 # serviceAccountName: default # volumes: # - name: nas001 # persistentVolumeClaim: # claimName: nas001 # volumeMounts: # - name: nas001 # mountPath: /nas001 Umcomment the property you want to override. For example, if you want your workflow jobs to run on GPU nodes, configure spec.resources or spec.nodeSelector property. spec: # image: jupyter/datascience-notebook:latest # imagePullPolicy: Always # imagePullSecrets: # - name: \"default\" # env: # - name: \"CUSTOM_KEY\" # value: \"CUSTOM_VAL\" resources: requests: cpu: 500m memory: 500Mi nvidia.com/gpu: 1 limits: cpu: 500m memory: 500Mi nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-node # runAsUser: 1000 # runAsGroup: 100 # serviceAccountName: default # volumes: # - name: nas001 # persistentVolumeClaim: # claimName: nas001 # volumeMounts: # - name: nas001 # mountPath: /nas001 Run jupyterflow and check out the result whether your workflow has run on GPU nodes. jupyterflow run -f workflow.yaml","title":"Jupyterflow Configuration"},{"location":"get-started/","text":"Get Started \u00b6 Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes. Prerequisite \u00b6 Kubernetes cluster Install kubectl command Install helm command Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes. Options for setting up JupyterFlow \u00b6 There are two ways to set up jupyterflow Set up JupyterFlow from scratch. Set up JupyterFlow on Kubeflow After setup, you can run your workflow with jupyterflow on JupyterHub. Launch your jupyter notebook and follow the instruction. Run my first workflow \u00b6 Refer to examples/get-started to get the example scripts. by command \u00b6 Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow. by workflow.yaml file \u00b6 If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your own workflow file (for example, workflow.yaml , the name doen't matter) For more information, check out Configuring workflow # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Get Started"},{"location":"get-started/#get-started","text":"Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes.","title":"Get Started"},{"location":"get-started/#prerequisite","text":"Kubernetes cluster Install kubectl command Install helm command Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes.","title":"Prerequisite"},{"location":"get-started/#options-for-setting-up-jupyterflow","text":"There are two ways to set up jupyterflow Set up JupyterFlow from scratch. Set up JupyterFlow on Kubeflow After setup, you can run your workflow with jupyterflow on JupyterHub. Launch your jupyter notebook and follow the instruction.","title":"Options for setting up JupyterFlow"},{"location":"get-started/#run-my-first-workflow","text":"Refer to examples/get-started to get the example scripts.","title":"Run my first workflow"},{"location":"get-started/#by-command","text":"Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow.","title":"by command"},{"location":"get-started/#by-workflowyaml-file","text":"If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your own workflow file (for example, workflow.yaml , the name doen't matter) For more information, check out Configuring workflow # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"by workflow.yaml file"},{"location":"how-it-works/","text":"How it works \u00b6 jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data) using given service account. With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code might help you understand how jupyterflow works. # collect meta data of current environment. notebook_pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build workflow based on meta data and user workflow information. workflow_spec = build_workflow ( notebook_pod_spec , user_workflow_file ) # create Argo workflow. response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data) using given service account. With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code might help you understand how jupyterflow works. # collect meta data of current environment. notebook_pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build workflow based on meta data and user workflow information. workflow_spec = build_workflow ( notebook_pod_spec , user_workflow_file ) # create Argo workflow. response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"kubeflow/","text":"Set up on Kubeflow \u00b6 What is Kubeflow? \u00b6 Kubeflow is a free and open-source machine learning platform designed to enable using machine learning pipelines to orchestrate complicated workflows running on Kubernetes. In this method, you will install JupyterFlow on existing Kubeflow platform. Install Kubeflow \u00b6 Refer to kubeflow getting started page for installation. Expose Argo Workflow UI \u00b6 Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow . Unfortunately, JupyterFlow currently does not support Kubeflow Pipelines, so the result of juypterflow Workflow does not appear in Kubeflow Pipelines Web pages. You need to manually expose Argo Workflow Web UI to check the result. Grant Kubeflow notebook Service Account RBAC \u00b6 Grant the service account used in Kubeflow notebook a role to create Argo Workflow objects. Options 1) \u00b6 The simplest way to grant service account is to bind cluster-admin role. The default service account name in Kubeflow notebook is default-editor . Assuming your Kubeflow namespace is jupyterflow , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default-editor Options 2) \u00b6 For more fine-grained RBAC, create Workflow Role in the namespace where Kubeflow is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default-editor service account with workflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default-editor \\ --namespace jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts . Install jupyterflow \u00b6 Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow","title":"Set up on Kubeflow"},{"location":"kubeflow/#set-up-on-kubeflow","text":"","title":"Set up on Kubeflow"},{"location":"kubeflow/#what-is-kubeflow","text":"Kubeflow is a free and open-source machine learning platform designed to enable using machine learning pipelines to orchestrate complicated workflows running on Kubernetes. In this method, you will install JupyterFlow on existing Kubeflow platform.","title":"What is Kubeflow?"},{"location":"kubeflow/#install-kubeflow","text":"Refer to kubeflow getting started page for installation.","title":"Install Kubeflow"},{"location":"kubeflow/#expose-argo-workflow-ui","text":"Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow . Unfortunately, JupyterFlow currently does not support Kubeflow Pipelines, so the result of juypterflow Workflow does not appear in Kubeflow Pipelines Web pages. You need to manually expose Argo Workflow Web UI to check the result.","title":"Expose Argo Workflow UI"},{"location":"kubeflow/#grant-kubeflow-notebook-service-account-rbac","text":"Grant the service account used in Kubeflow notebook a role to create Argo Workflow objects.","title":"Grant Kubeflow notebook Service Account RBAC"},{"location":"kubeflow/#options-1","text":"The simplest way to grant service account is to bind cluster-admin role. The default service account name in Kubeflow notebook is default-editor . Assuming your Kubeflow namespace is jupyterflow , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default-editor","title":"Options 1)"},{"location":"kubeflow/#options-2","text":"For more fine-grained RBAC, create Workflow Role in the namespace where Kubeflow is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default-editor service account with workflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default-editor \\ --namespace jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts .","title":"Options 2)"},{"location":"kubeflow/#install-jupyterflow","text":"Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow","title":"Install jupyterflow"},{"location":"scratch/","text":"Set up from scratch \u00b6 In this method, you will install JupyterHub, Argo Workflow manually. Install JupyterHub \u00b6 Follow the Zero to JupyterHub instruction to set up JupyterHub . There is two things you should configure while installing jupyterflow. 1) Specify serviceAccoutName \u00b6 Find singleuser property and specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account a role to create Workflow object. # config.yaml singleuser : serviceAccountName : default 2) Configure Storage \u00b6 To use the same JupyterHub home directory as in Argo Workflow, Configure singleuser.storage property. To run jobs on multiple different node, you should use ReadWriteMany access mode type storage, such as nfs-server-provisioner . If you're unfamiliar with storage access mode, take a look at Kubernetes persistent volume access mode . Configuring singleuser storage access mode as ReadWriteOnce is perfectly fine, but bear in mind that your jobs will be run on the only one node that your jupyter notebook is mounted. # config.yaml singleuser : storage : type : dynamic # or static dynamic : storageClass : nfs-server # For example, nfs-server-provisioner storageAccessModes : [ ReadWriteMany ] # Make sure your volume supports ReadWriteMany for running distributed jobs. static : pvcName : my-static-pvc # Static pvc also works fine. # Also static pvc should support ReadWriteMany mode for distributed jobs. The full description of config.yaml file will seem like this. proxy : secretToken : \"<RANDOM_HEX>\" singleuser : serviceAccountName : default storage : type : dynamic dynamic : storageClass : nfs-server storageAccessModes : [ ReadWriteMany ] Install JupyterHub using helm package manager. Following example installs JupyterHub in jupyterflow namespace. helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ helm repo update RELEASE = jhub NAMESPACE = jupyterflow helm install $RELEASE jupyterhub/jupyterhub \\ --namespace $NAMESPACE \\ --create-namespace \\ --values config.yaml Install Argo Workflow Engine \u00b6 Install Argo workflow engine with Argo Workflow quick start page . You need to install Argo workflow engine in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for Argo Workflow engine. # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml If you want to install Argo workflow engine in different namespace, refer to Argo installation page. Expose Argo Workflow UI \u00b6 Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow . Grant JupyterHub Service Account RBAC \u00b6 Grant the service account used in JupyterHub a role to create Argo Workflow objects. Options 1) \u00b6 The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default Options 2) \u00b6 For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default service account with workflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ --namespace jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts . Install jupyterflow \u00b6 Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow","title":"Set up from scratch"},{"location":"scratch/#set-up-from-scratch","text":"In this method, you will install JupyterHub, Argo Workflow manually.","title":"Set up from scratch"},{"location":"scratch/#install-jupyterhub","text":"Follow the Zero to JupyterHub instruction to set up JupyterHub . There is two things you should configure while installing jupyterflow.","title":"Install JupyterHub"},{"location":"scratch/#1-specify-serviceaccoutname","text":"Find singleuser property and specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account a role to create Workflow object. # config.yaml singleuser : serviceAccountName : default","title":"1) Specify serviceAccoutName"},{"location":"scratch/#2-configure-storage","text":"To use the same JupyterHub home directory as in Argo Workflow, Configure singleuser.storage property. To run jobs on multiple different node, you should use ReadWriteMany access mode type storage, such as nfs-server-provisioner . If you're unfamiliar with storage access mode, take a look at Kubernetes persistent volume access mode . Configuring singleuser storage access mode as ReadWriteOnce is perfectly fine, but bear in mind that your jobs will be run on the only one node that your jupyter notebook is mounted. # config.yaml singleuser : storage : type : dynamic # or static dynamic : storageClass : nfs-server # For example, nfs-server-provisioner storageAccessModes : [ ReadWriteMany ] # Make sure your volume supports ReadWriteMany for running distributed jobs. static : pvcName : my-static-pvc # Static pvc also works fine. # Also static pvc should support ReadWriteMany mode for distributed jobs. The full description of config.yaml file will seem like this. proxy : secretToken : \"<RANDOM_HEX>\" singleuser : serviceAccountName : default storage : type : dynamic dynamic : storageClass : nfs-server storageAccessModes : [ ReadWriteMany ] Install JupyterHub using helm package manager. Following example installs JupyterHub in jupyterflow namespace. helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ helm repo update RELEASE = jhub NAMESPACE = jupyterflow helm install $RELEASE jupyterhub/jupyterhub \\ --namespace $NAMESPACE \\ --create-namespace \\ --values config.yaml","title":"2) Configure Storage"},{"location":"scratch/#install-argo-workflow-engine","text":"Install Argo workflow engine with Argo Workflow quick start page . You need to install Argo workflow engine in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for Argo Workflow engine. # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml If you want to install Argo workflow engine in different namespace, refer to Argo installation page.","title":"Install Argo Workflow Engine"},{"location":"scratch/#expose-argo-workflow-ui","text":"Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow .","title":"Expose Argo Workflow UI"},{"location":"scratch/#grant-jupyterhub-service-account-rbac","text":"Grant the service account used in JupyterHub a role to create Argo Workflow objects.","title":"Grant JupyterHub Service Account RBAC"},{"location":"scratch/#options-1","text":"The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default , run # binding cluster-admin role to jupyterflow:default kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default","title":"Options 1)"},{"location":"scratch/#options-2","text":"For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. For example, create Workflow Role in jupyterflow namespace with following command. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch - list # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch - apiGroups: - \"argoproj.io\" resources: - workflows verbs: - get - watch - patch - list - create EOF Then, bind Role with your service account. For example, bind default service account with workflow role in jupyterflow namespace. # binding workflow role to jupyterflow:default kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ --namespace jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts .","title":"Options 2)"},{"location":"scratch/#install-jupyterflow","text":"Finally, launch a JupyterHub notebook server and install jupyterflow using pip. In jupyter notebook Terminal, run pip install jupyterflow","title":"Install jupyterflow"},{"location":"examples/","text":"Examples \u00b6 Basic \u00b6 Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook. ML Pipeline \u00b6 ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#basic","text":"Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook.","title":"Basic"},{"location":"examples/#ml-pipeline","text":"ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"ML Pipeline"},{"location":"examples/basic/","text":"Basic \u00b6 Run by command \u00b6 Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple execution. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow. Run by workflow.yaml \u00b6 Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - ls - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Basic"},{"location":"examples/basic/#basic","text":"","title":"Basic"},{"location":"examples/basic/#run-by-command","text":"Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple execution. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow.","title":"Run by command"},{"location":"examples/basic/#run-by-workflowyaml","text":"Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - ls - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Run by workflow.yaml"},{"location":"examples/ml-pipeline/","text":"ML Pipeline \u00b6 Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for testing purpose. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python input.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"},{"location":"examples/ml-pipeline/#ml-pipeline","text":"Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for testing purpose. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python input.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"}]}