{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JupyterFlow Documentation \u00b6 Run your workflow on JupyterHub! What is JupyterFlow? \u00b6 Run Argo Workflow pipeline on JupyterHub . No Kubernetes knowledge (YAML) needed to run. No container build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. The following jupyterflow command will make sequence workflow. jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file Problem to solve \u00b6 I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who are not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem. Getting Started \u00b6 To set up jupyterflow and start running your first workflow, follow the Getting Started guide. How does it work \u00b6 To learn how it works, go to How it works guide. Examples \u00b6 For examples how to use, please see Examples page. Workflow file Configuration \u00b6 To find out more configuration, take a look at Configuration page.","title":"Overview"},{"location":"#jupyterflow-documentation","text":"Run your workflow on JupyterHub!","title":"JupyterFlow Documentation"},{"location":"#what-is-jupyterflow","text":"Run Argo Workflow pipeline on JupyterHub . No Kubernetes knowledge (YAML) needed to run. No container build & push or deploy. Just simply run your workflow with single command jupyterflow . jupyterflow is a command that helps user utilize Argo Workflow engine without making any YAML files or building containers on JupyterHub. The following jupyterflow command will make sequence workflow. jupyterflow run -c \"python hello.py >> python world.py\" To make parallel workflow, write your own workflow.yaml file","title":"What is JupyterFlow?"},{"location":"#problem-to-solve","text":"I wanted to train multiple ML models efficiently. Using Kubernetes was a good idea, since it is easy to make distributed jobs. it is easy to schedule ML jobs on multiple training server. it has native resource management mechanism. it has good monitoring system. But there were some drawbacks. I needed to re-build & re-push image everytime I updated my model. This was painful. People who are not familiar with k8s had a hard time using this method. jupyterflow aims to solve this problem.","title":"Problem to solve"},{"location":"#getting-started","text":"To set up jupyterflow and start running your first workflow, follow the Getting Started guide.","title":"Getting Started"},{"location":"#how-does-it-work","text":"To learn how it works, go to How it works guide.","title":"How does it work"},{"location":"#examples","text":"For examples how to use, please see Examples page.","title":"Examples"},{"location":"#workflow-file-configuration","text":"To find out more configuration, take a look at Configuration page.","title":"Workflow file Configuration"},{"location":"configuration/","text":"Configuring workflow file \u00b6 workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs. Minimum description \u00b6 Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again Full description \u00b6 Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately","title":"Configuring workflow file"},{"location":"configuration/#configuring-workflow-file","text":"workflow.yaml is a description about how to run your jobs. You can asign a dependencies between jobs.","title":"Configuring workflow file"},{"location":"configuration/#minimum-description","text":"Each job will run in parallel. # workflow.yaml jobs : - echo hello - echo world - echo again","title":"Minimum description"},{"location":"configuration/#full-description","text":"Job dependencies will be resolved based on dags information. # workflow.yaml version : 1 name : workflow-name jobs : - echo hello - echo world - echo again dags : - 1 >> 2 - 1 >> 3 schedule : '*/2 * * * *' Property Description Optional Default version Version of workflow.yaml file format. Optional 1 name Name of the workflow. This name is used for Argo Workflow name. Optional {username} of JupyterHub jobs Jobs to run. Any kinds of command works. Required dags Job dependencies. Index starts at 1. ( $PREVIOUS_JOB >> $NEXT_JOB ) Optional All jobs parallel (No dependency) schedule When to execute this workflow. Follows cron format. Optional Run immediately","title":"Full description"},{"location":"get-started/","text":"Get Started \u00b6 Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes. 1. Install Kubernetes \u00b6 Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes. 2. Install JupyterHub \u00b6 Also, follow the Zero to JupyterHub instruction to set up JupyterHub. There is one thing you should be aware of while installing jupyterflow. Specify serviceAccoutName \u00b6 You need to specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account to create Workflow object. # config.yaml singleuser : serviceAccountName : default 3. Install Argo Workflow \u00b6 Install Argo workflow with this page You need to install Argo workflow in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for JupyterHub and Argo Workflow. # create namespace jupyterflow kubectl create ns jupyterflow # install jupyterhub in jupyterflow helm install jupyterhub jupyterhub/jupyterhub --namespace jupyterflow # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml 4. Expose Argo Workflow UI \u00b6 Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow . 5. Grant JupyterHub ServiceAccount RBAC \u00b6 Grant service account used in JupyterHub the ability to create Argo Workflow objects. Options 1) \u00b6 The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default # --serviceaccount=<NAMESPACE>:<SERVICE_ACCOUNT> kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default Options 2) \u00b6 For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch EOF Then, bind Role with your service account. For example, binding default service account in jupyterflow namespace. kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ -n jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts 6. Install jupyterflow \u00b6 Finally, launch a JupyterHub notebook server and install jupyterflow using pip. pip install jupyterflow 7. Run my first workflow \u00b6 Refer to examples/get-started Run by command \u00b6 Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow. Run by workflow.yaml \u00b6 If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your workflow on file (for example, workflow.yaml , the name doen't matter) # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Get Started"},{"location":"get-started/#get-started","text":"Although using jupyterflow does not require Kubernetes knowledge, Setting up jupyterflow requires Kubernetes knowledge(YAML, helm , Service ). If you're familiar with Kubernetes, it will not be too hard. This project only works on JupyterHub for Kubernetes.","title":"Get Started"},{"location":"get-started/#1-install-kubernetes","text":"Any Kubernetes distributions will work. Zero to JupyterHub has a wonderful guide for setting up Kubernetes.","title":"1. Install Kubernetes"},{"location":"get-started/#2-install-jupyterhub","text":"Also, follow the Zero to JupyterHub instruction to set up JupyterHub. There is one thing you should be aware of while installing jupyterflow.","title":"2. Install JupyterHub"},{"location":"get-started/#specify-serviceaccoutname","text":"You need to specify serviceAccoutName in config.yaml . This service account will be used to create Argo Workflow object on behalf of you. For example, use default service account. Later, you should grant this service account to create Workflow object. # config.yaml singleuser : serviceAccountName : default","title":"Specify serviceAccoutName"},{"location":"get-started/#3-install-argo-workflow","text":"Install Argo workflow with this page You need to install Argo workflow in the same Kubernetes namespace where JupyterHub is installed. For example, using jupyterflow namespace for JupyterHub and Argo Workflow. # create namespace jupyterflow kubectl create ns jupyterflow # install jupyterhub in jupyterflow helm install jupyterhub jupyterhub/jupyterhub --namespace jupyterflow # install argo workflow in jupyterflow kubectl apply --namespace jupyterflow -f \\ https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml","title":"3. Install Argo Workflow"},{"location":"get-started/#4-expose-argo-workflow-ui","text":"Expose Web UI for Argo Workflow: https://argoproj.github.io/argo/argo-server/ You need to expose Argo Web UI to see the result of jupyterflow .","title":"4. Expose Argo Workflow UI"},{"location":"get-started/#5-grant-jupyterhub-serviceaccount-rbac","text":"Grant service account used in JupyterHub the ability to create Argo Workflow objects.","title":"5. Grant JupyterHub ServiceAccount RBAC"},{"location":"get-started/#options-1","text":"The simplest way to grant service account is to bind cluster-admin role. For example, if you deployed JupyterHub in jupyterflow namespace and specify service account as default # --serviceaccount=<NAMESPACE>:<SERVICE_ACCOUNT> kubectl create clusterrolebinding jupyterflow-admin \\ --clusterrole = cluster-admin \\ --serviceaccount = jupyterflow:default","title":"Options 1)"},{"location":"get-started/#options-2","text":"For more fine-grained RBAC, create Workflow Role in the namespace where JupyterHub is installed. cat << EOF | kubectl create -n jupyterflow -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch EOF Then, bind Role with your service account. For example, binding default service account in jupyterflow namespace. kubectl create rolebinding workflow-rb \\ --role = workflow-role \\ --serviceaccount = jupyterflow:default \\ -n jupyterflow You might want to look at https://argoproj.github.io/argo/service-accounts","title":"Options 2)"},{"location":"get-started/#6-install-jupyterflow","text":"Finally, launch a JupyterHub notebook server and install jupyterflow using pip. pip install jupyterflow","title":"6. Install jupyterflow"},{"location":"get-started/#7-run-my-first-workflow","text":"Refer to examples/get-started","title":"7. Run my first workflow"},{"location":"get-started/#run-by-command","text":"Write your own code in notebook server. # job1.py print ( 'hello' ) # job2.py import sys print ( 'world %s !' % sys . argv [ 1 ]) Run following command for sequence workflow. jupyterflow run -c \"python job1.py >> python job2.py foo\" Go to Argo Web UI and check out the output of launched workflow.","title":"Run by command"},{"location":"get-started/#run-by-workflowyaml","text":"If you want to run more sophisticated workflow, such as DAG (Directed Acyclic Graph), write your workflow on file (for example, workflow.yaml , the name doen't matter) # workflow.yaml jobs : - python job1.py - python job2.py foo - python job2.py bar - python job3.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 2 >> 4 - 3 >> 4 # job3.py print ( 'again!' ) Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Run by workflow.yaml"},{"location":"how-it-works/","text":"How it works \u00b6 jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data). With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code helps you understand how jupyterflow works. # collect meta data of current environment. pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build Workflow based on meta data and user workflow information workflow_spec = build_workflow ( pod_spec , user_workflow_file ) # create new workflow response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"jupyterflow has a strict constraint that it only works on JupyterHub for Kubernetes . Because of this constraint, jupyterflow can easily collect current environment information (meta data). With this information, jupyterflow constructs Argo Workflow object on behalf of you. jupyterflow uses following metadata from jupyter notebook Pod . Container image Environment variables Home directory (home PersistentVolumeClaim ) Extra volume mount points Resource management ( requests , limits ) UID, GUID Following pseudo code helps you understand how jupyterflow works. # collect meta data of current environment. pod_spec = get_current_pod_spec_from_k8s ( pod_name , service_account ) # build Workflow based on meta data and user workflow information workflow_spec = build_workflow ( pod_spec , user_workflow_file ) # create new workflow response = request_for_new_workflow_to_k8s ( workflow_spec , service_account )","title":"How it works"},{"location":"examples/","text":"Examples \u00b6 Basic \u00b6 Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook. ML Pipeline \u00b6 ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#basic","text":"Basic example for beginners. It shows any type of command is possible to run only if it's install in your jupyter notebook.","title":"Basic"},{"location":"examples/#ml-pipeline","text":"ML pipeline example for machine learning experiments. You can run various ML train job easily with jupyterflow command. This is the one of the main reason why you should use jupyterflow .","title":"ML Pipeline"},{"location":"examples/basic/","text":"Basic \u00b6 Run by command \u00b6 Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple command. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow. Run by workflow.yaml \u00b6 Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - pwd - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Basic"},{"location":"examples/basic/#basic","text":"","title":"Basic"},{"location":"examples/basic/#run-by-command","text":"Clone jupyterflow git repository and go to examples/basic . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/basic ls -alh # hello.sh # workflow.yaml hello.sh : Script to run in workflow. workflow.yaml : workflow file hello.sh script looks like this. it simply echos argument. # hello.sh echo \"Hello $1 \" Run jupyterflow with -c option for simple command. jupyterflow run -c \"bash hello.sh world >> echo 'good bye'\" Go to Argo Web UI and check out the output of launched workflow.","title":"Run by command"},{"location":"examples/basic/#run-by-workflowyaml","text":"Write workflow.yaml for parallel execution. # workflow.yaml jobs : - bash hello.sh world - bash hello.sh bob - bash hello.sh foo - pwd - echo 'jupyterflow is the best!' # Job index starts at 1. dags : - 1 >> 4 - 2 >> 4 - 3 >> 4 - 4 >> 5 Run jupyteflow with -f option. jupyterflow run -f workflow.yaml Check out the result.","title":"Run by workflow.yaml"},{"location":"examples/ml-pipeline/","text":"ML Pipeline \u00b6 Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for test. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python intput.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"},{"location":"examples/ml-pipeline/#ml-pipeline","text":"Clone jupyterflow git repository and go to examples/ml-pipeline . git clone https://github.com/hongkunyoo/jupyterflow.git cd examples/ml-pipeline ls -alh # input.py # train.py # output.py # workflow.yaml # requirements.txt input.py : Script for preparing train data. train.py : Model training experiments. output.py : Scores trained models. workflow.yaml : jupyterflow workflow file requirements.txt : Pip packages for ML pipeline First, install required packages. pip install -r requirements.txt Run each script in jupyter notebook for test. python input.py python train.py softmax 0 .5 python output.py Write various training experiments to find the best performing model. # workflow.yaml jobs : - python intput.py - python train.py softmax 0.5 - python train.py softmax 0.9 - python train.py relu 0.5 - python train.py relu 0.9 - python output.py # Job index starts at 1. dags : - 1 >> 2 - 1 >> 3 - 1 >> 4 - 1 >> 5 - 2 >> 6 - 3 >> 6 - 4 >> 6 - 5 >> 6 Run your ML Pipeline. jupyterflow run -f workflow.yaml Check out the result in Argo Web UI.","title":"ML Pipeline"}]}